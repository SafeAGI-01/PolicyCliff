\section{Conclusion}\label{sec-conclusion}

The reliable alignment and reasoning of large language models is fundamentally constrained by the instability of reinforcement learning, which often yields brittle policies that fail in unpredictable ways. This work confronted this challenge not with more sophisticated heuristics, but by establishing a foundational mathematical theory of policy stability. Our central contribution is the formal proof that policy instability is not an empirical accident but a direct and predictable mathematical consequence of reward perturbation and degenerate optimal action sets, which are a common feature in the vast, nuanced decision spaces of language and reasoning.

This theoretical principle provided a unified lens, reframing a wide range of seemingly disparate alignment failures, from the evolution of deceptive reasoning to systemic intelligence-obedience trade-offs and RLHF-induced sophistry, as rational and predictable outcomes of policy optimization on incomplete or ambiguous reward landscapes. We extended this framework from the foundational single-reward setting to the more complex multi-reward paradigm across diverse domains, demonstrating that stability then hinges critically on the mechanism for aggregating disparate reward signals into an "effective reward". Finally, moving from diagnosis to prescription, we formally guaranteed a solution by proving that entropy regularization robustly restores Lipschitz continuity to the reward-policy map, thereby ensuring a smooth and stable policy response to changes in the underlying reward models.

This work reframes policy stability from reactive heuristics to a principled theory. It introduces a formal framework to analyze and anticipate instabilities in AI systems, laying the groundwork for more robust reinforcement learning. While the analysis provides essential foundations, it also highlights key directions for future research, especially the stability of dynamically learned reward aggregation. By establishing the mathematical perspective for stability, this research advances the design of RL algorithms that prioritize not only optimality but also the reliability essential for safe and trustworthy AI.
