\section{LLMs Trained with Multiple Specialized Reward Models}\label{sec:llm-multi-reward}

Large Language Models (LLMs) are increasingly trained to handle diverse tasks and exhibit nuanced behaviors. A sophisticated approach to achieve this involves using multiple, specialized reward models, each tailored to a specific class of data or desired capability. This section extends our continuity analysis to such a multi-reward RL training paradigm across diverse domains (see e.g., \cite{liang2025modomodo,cheng2025revisiting,su2025crossing}).

\subsection{Framework: Single LLM, Multiple Data Classes and Reward Models}

Consider a scenario where a single LLM policy, $\Policy: \StateSpace \to \PolicySpace(\ActionSpace)$, is trained to perform well across $N$ distinct classes of data or tasks. Let these classes be denoted by $k \in \{1, \ldots, N\}$. The framework is shown in Figure~\ref{fig:multi-reward-framework}.
\begin{itemize}
    \item \textbf{Data Classes ($\mathcal{D}_k$):} Each class $\mathcal{D}_k$ represents a distribution of initial states (e.g., prompts) specific to a certain domain or task (e.g., mathematical reasoning, coding, question answering, safety alignment).
    \item \textbf{Specialized Reward Models ($\RewardFunc_k$):} For each data class $\mathcal{D}_k$, there is a corresponding reward model that provides a reward function $\RewardFunc_k: \StateSpace \times \ActionSpace \to \R$. This function $\RewardFunc_k(s,a)$ evaluates the quality of action $a$ (generating a token) in state $s$ (current sequence) specifically for task $k$. We assume each $\RewardFunc_k \in \RewardSpace = \Cont(\StateSpace \times \ActionSpace)$. Let $\boldsymbol{\RewardFunc} = (\RewardFunc_1, \ldots, \RewardFunc_N)$ denote the tuple of these reward functions. The space of such tuples is $\RewardSpace^N$, equipped with the norm ${\SupNorm{\boldsymbol{\RewardFunc} - \boldsymbol{\RewardFunc}'}}_{N} = \max_{1 \le k \le N} \SupNorm{\RewardFunc_k - \RewardFunc_k'}$.
    \item \textbf{Training Objective:} The LLM policy $\Policy$ is trained to optimize a global objective that aggregates performance across all $N$ task-reward pairs. This is often formulated as maximizing the weighted sum of expected discounted rewards. During training, if an initial state $s_0$ is sampled from $\mathcal{D}_k$ (with probability $p_k$, where $\sum_{k=1}^N p_k = 1$ and we assume $p_k > 0$ for all $k$), then the subsequent rewards for the trajectory generated by $\Policy$ are drawn from $\RewardFunc_k$. The overall objective is to find a policy $\OptPolicy_{\boldsymbol{\RewardFunc}}$ that maximizes:
    \begin{equation} \label{eq:multi_objective_revised}
        J\left(\Policy; \boldsymbol{\RewardFunc}, \{\mathcal{D}_k\}, \{p_k\}\right) = \sum_{k=1}^N p_k \mathbb{E}_{s_0 \sim \mathcal{D}_k, \tau \sim \Policy(\cdot|s_0)} \left[ \sum_{t=0}^\infty \gamma^t \RewardFunc_k(s_t, a_t) \right].
    \end{equation}
    Here, $\tau \sim \Policy(\cdot|s_0)$ denotes a trajectory generated by policy $\Policy$ starting from $s_0$.
\end{itemize}
The state space $\StateSpace$, action space $\ActionSpace$, transition kernel $\TransKernel$, and discount factor $\gamma$ are defined as in Section \ref{sec-rl} (satisfying Assumptions \ref{assump:spaces}, \ref{assump:transition}, and \ref{assump:discount}). The single policy $\Policy$ must learn to adapt its behavior based on the implicit context of the input state $s$, even though it may not explicitly receive the class index $k$ as input.

\input{figures/multi-reward-framework}


\subsection{Analyzing Policies Derived from State-Dependent Effective Rewards}

The global objective $J(\Policy)$ in Eq.~\eqref{eq:multi_objective_revised} averages performance across episodes that are governed by distinct, episode-specific reward functions $\RewardFunc_k$. A single policy $\Policy(a|s)$, lacking explicit knowledge of $k$, must infer the context from $s$. The direct characterization of $\OptPolicy_{\boldsymbol{\RewardFunc}}$ maximizing $J(\Policy)$ via a simple Bellman optimality equation (as in Section \ref{sec-rl}) is generally not available.

To apply the continuity analysis tools, we focus on a structured model of how an LLM might address this multi-task challenge: by forming an internal, state-dependent \emph{effective reward function}, $\RewardFunc_{eff}$. This function aggregates the specialized rewards based on the current context:
\begin{equation} \label{eq:effective_reward}
    \RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}) = \sum_{k=1}^N w_k(s) \RewardFunc_k(s,a).
\end{equation}
Here, $w_k: \StateSpace \to [0,1]$ are continuous weighting functions satisfying $\sum_{k=1}^N w_k(s) = 1$ for all $s \in \StateSpace$. These weights model the LLM's assessment of the relevance of task $k$ given state $s$. The following analysis assumes that the weighting functions $w_k(s)$ are given. Crucially, these $w_k(s)$ are assumed to be fixed and do not change as $\boldsymbol{\RewardFunc}$ is perturbed. If $w_k(s)$ were themselves functions of $\boldsymbol{\RewardFunc}$, a more comprehensive stability analysis would be required.

The policy subject to our continuity analysis, denoted ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$, is the optimal policy for the standard MDP defined by this $\RewardFunc_{eff}$. It is thus greedy with respect to the optimal action-value function $\OptQFunc_{eff}(s,a; \boldsymbol{\RewardFunc})$, which is the unique fixed point of the Bellman optimality equation:
\begin{equation} \label{eq:bellman_effective}
(\BellmanOp_{eff, \boldsymbol{\RewardFunc}} Q)(s, a) = \RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}) + \gamma \int_{\StateSpace} \max_{a' \in \ActionSpace} Q(s', a') \TransKernel(ds' | s, a).
\end{equation}
The optimal policy ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ has its support contained within $A_{eff}^*(s; \boldsymbol{\RewardFunc}) = \Argmax_{a \in \ActionSpace} \OptQFunc_{eff}(s, a; \boldsymbol{\RewardFunc})$.

\begin{remark}[On the Relationship between ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ and $J(\Policy)$]
The policy ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ analyzed here is optimal for the constructed $\RewardFunc_{eff}$. The alignment of ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ with $\OptPolicy_{\boldsymbol{\RewardFunc}}$ (the maximizer of $J(\Policy)$) depends on how well $w_k(s)$ are chosen or learned. If, for instance, the initial state distributions were identical ($\mathcal{D}_k = \mathcal{D}_{init}$ for all $k$) and one chose $w_k(s) = p_k$ (constant weights), then $J(\Policy)$ simplifies to $\mathbb{E}_{s_0 \sim \mathcal{D}_{init}} [V_{\RewardFunc_{eff}}^{\Policy}(s_0)]$, and ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ would indeed be $\OptPolicy_{\boldsymbol{\RewardFunc}}$.
Our analysis in this section focuses on the stability of ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ resulting from such an effective reward mechanism.
\end{remark}

\begin{lemma}[Properties of the Effective Reward Mapping]\label{lemma:effective_reward_properties}
Let each $\RewardFunc_k \in \RewardSpace$, and let $w_k: \StateSpace \to [0,1]$ be continuous functions such that $\sum_{k=1}^N w_k(s) = 1$ for all $s \in \StateSpace$.
\begin{enumerate}
    \item For any $\boldsymbol{\RewardFunc} \in \RewardSpace^N$, the effective reward function $\RewardFunc_{eff}(\cdot, \cdot; \boldsymbol{\RewardFunc})$ defined in Eq.~\eqref{eq:effective_reward} is continuous on $\StateSpace \times \ActionSpace$.
    \item The mapping $\boldsymbol{\RewardFunc} \mapsto \RewardFunc_{eff}(\cdot, \cdot; \boldsymbol{\RewardFunc})$ is Lipschitz continuous from $(\RewardSpace^N, {\SupNorm{\cdot}}_{N})$ to $(\RewardSpace, \SupNorm{\cdot})$ with Lipschitz constant 1:
    \[ \SupNorm{\RewardFunc_{eff}(\cdot,\cdot;\boldsymbol{\RewardFunc}) - \RewardFunc_{eff}(\cdot,\cdot;\boldsymbol{\RewardFunc}')} \le {\SupNorm{\boldsymbol{\RewardFunc} - \boldsymbol{\RewardFunc}'}}_{N}. \]
\end{enumerate}
\end{lemma}
\begin{proof}
Since each $\RewardFunc_k$ is continuous on $\StateSpace \times \ActionSpace$ and each $w_k$ is continuous on $\StateSpace$, their products $w_k(s)\RewardFunc_k(s,a)$ are continuous. The finite sum $\RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}) = \sum_{k=1}^N w_k(s) \RewardFunc_k(s,a)$ is therefore continuous. 
For any $(s,a) \in \StateSpace \times \ActionSpace$, 
\begin{align*}
|\RewardFunc_{eff}(s,a;\boldsymbol{\RewardFunc}) - \RewardFunc_{eff}(s,a;\boldsymbol{\RewardFunc}')| &= \left| \sum_{k=1}^N w_k(s) (\RewardFunc_k(s,a) - \RewardFunc_k'(s,a)) \right| \\
&\le \sum_{k=1}^N w_k(s) |\RewardFunc_k(s,a) - \RewardFunc_k'(s,a)| \\
&\le \sum_{k=1}^N w_k(s) \SupNorm{\RewardFunc_k - \RewardFunc_k'} \\
&\le \left( \sum_{k=1}^N w_k(s) \right) \max_{1 \le j \le N} \SupNorm{\RewardFunc_j - \RewardFunc_j'} \\
&= 1 \cdot {\SupNorm{\boldsymbol{\RewardFunc} - \boldsymbol{\RewardFunc}'}}_{N}.
\end{align*}
Taking the supremum over $(s,a)$ on the left-hand side yields the result.
\end{proof}

\subsection{Continuity of the Policy Map for the Effective Reward Model}\label{subsec:cont_eff_reward}

We now analyze the continuity of ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ with respect to $\boldsymbol{\RewardFunc}$, based on the properties of $\RewardFunc_{eff}$.

\begin{proposition}[Lipschitz Stability of Effective Q-Function]\label{prop:lipschitz-effective-q-revised}
Under Assumptions \ref{assump:spaces}-\ref{assump:discount} and the existence of continuous $w_k(s)$ as defined above, the mapping $\boldsymbol{\RewardFunc} \mapsto \OptQFunc_{eff}(\cdot,\cdot;\boldsymbol{\RewardFunc})$ from $(\RewardSpace^N, {\SupNorm{\cdot}}_{N})$ to $(\Cont(\StateSpace \times \ActionSpace), \SupNorm{\cdot})$ is Lipschitz continuous with constant $1/(1-\gamma)$.
\end{proposition}
\begin{proof}
This follows from Proposition \ref{prop:q_continuity} and Lemma \ref{lemma:effective_reward_properties}.
\end{proof}

\begin{lemma}[Upper Hemi-Continuity of Effective Argmax Correspondence]\label{lemma:effective_argmax_uhc-revised}
Under Assumptions \ref{assump:spaces}-\ref{assump:discount} and the existence of continuous $w_k(s)$, for each fixed $s \in \StateSpace$, the argmax correspondence $\Phi_s^{eff}: \RewardSpace^N \twoheadrightarrow \ActionSpace$ defined by $\Phi_s^{eff}(\boldsymbol{\RewardFunc}) = A_{eff}^*(s; \boldsymbol{\RewardFunc})$ is non-empty, compact-valued, and upper hemi-continuous.
\end{lemma}
\begin{proof}
The function $\OptQFunc_{eff}(s,a;\boldsymbol{\RewardFunc})$ is continuous in $(s,a)$ (as it is the fixed point of $\BellmanOp_{eff, \boldsymbol{\RewardFunc}}$ which maps continuous functions to continuous functions, given $\RewardFunc_{eff}$ is continuous by Lemma \ref{lemma:effective_reward_properties}). By Proposition \ref{prop:lipschitz-effective-q-revised}, $\OptQFunc_{eff}$ is continuous with respect to $\boldsymbol{\RewardFunc}$. The proof is then similar to that of Lemma~\ref{lemma:argmax_uhc}.
\end{proof}

\begin{proposition}[Continuity of Policy Map (Effective Model) under Uniqueness]\label{thm:multi-continuity-uniqueness-eff}
Let Assumptions \ref{assump:spaces}-\ref{assump:discount} and the existence of continuous $w_k(s)$ hold. Let $\boldsymbol{\RewardFunc}_0 \in \RewardSpace^N$. Suppose there exists an open neighborhood $\mathcal{N}(\boldsymbol{\RewardFunc}_0) \subset \RewardSpace^N$ of $\boldsymbol{\RewardFunc}_0$ such that for all $\boldsymbol{\RewardFunc} \in \mathcal{N}(\boldsymbol{\RewardFunc}_0)$ and all $s \in \StateSpace$, $A_{eff}^*(s; \boldsymbol{\RewardFunc})$ is a singleton, denoted $\{a_{eff}^*(s; \boldsymbol{\RewardFunc})\}$.
Define the policy map $M_{RL}^{eff}: \mathcal{N}(\boldsymbol{\RewardFunc}_0) \to \PolicySpace_{det}$ by $M_{RL}^{eff}(\boldsymbol{\RewardFunc})(s) = a_{eff}^*(s; \boldsymbol{\RewardFunc})$. This map defines the deterministic selections of ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$.
If $\PolicySpace_{det}$ is equipped with the topology of pointwise convergence, then $M_{RL}^{eff}$ is continuous at $\boldsymbol{\RewardFunc}_0$.
\end{proposition}
\begin{proof}
The proof is analogous to that of Theorem \ref{thm:continuity_rigorous}, using $\RewardFunc_{eff}$ and Lemma \ref{lemma:effective_argmax_uhc-revised}.
\end{proof}

\begin{proposition}[Discontinuity of Policy Map (Effective Model) under Non-Uniqueness]\label{prop:multi-discontinuity-eff}
Let Assumptions \ref{assump:spaces}-\ref{assump:discount} and the existence of continuous $w_k(s)$ hold. Suppose for $\boldsymbol{\RewardFunc}_0 \in \RewardSpace^N$, there exists $s_0 \in \StateSpace$ such that $A_{eff}^*(s_0; \boldsymbol{\RewardFunc}_0)$ is finite and contains at least two distinct actions, $a_1 \neq a_2$. 

(1) Let $M_{RL}^{eff}: \RewardSpace^N \to \PolicySpace_{det}$ be a policy map selecting a deterministic policy ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}(s) \in A_{eff}^*(s; \boldsymbol{\RewardFunc})$ (e.g., $M_{RL}^{eff}(\boldsymbol{\RewardFunc}_0)(s_0) = a_1$), then $M_{RL}^{eff}$ is discontinuous at $\boldsymbol{\RewardFunc}_0$ under pointwise convergence. 

(2) Let $M_{RL}^{eff}: \RewardSpace^N \to \PolicySpace$ be defined by selecting the stochastic policy ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$ such that for each $s \in \StateSpace$, ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}(\cdot | s)$ is the uniform probability distribution over the set $A_{eff}^*(s; \boldsymbol{\RewardFunc})$. 
Then the map $\boldsymbol{\RewardFunc} \mapsto {\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}(\cdot | s_0)$, viewed as a map from $(\RewardSpace^N, {\SupNorm{\cdot}}_{N})$ to $(\PolicySpace, d_{TV})$ is discontinuous at $\boldsymbol{\RewardFunc}_0$.
\end{proposition}
\begin{proof}
This proof applies the "inverse Bellman" method to the effective Q-function, $\OptQFunc_{eff}$, to demonstrate discontinuity. The core idea is to first construct a perturbed effective Q-function, $\QFunc_{eff, \varepsilon}$, that guarantees an action switch, and then construct a tuple of reward functions, $\boldsymbol{\RewardFunc}_\varepsilon$, that is guaranteed to produce this perturbed Q-function and that converges to $\boldsymbol{\RewardFunc}_0$.

\noindent\textbf{1. Construct a Perturbation in the Effective Q-Function Space}

Let $\RewardFunc_{eff,0}(s,a) = \sum_{k=1}^N w_k(s) \RewardFunc_{k,0}(s,a)$ be the initial effective reward. Let $Q_{eff,0} = \OptQFunc_{\RewardFunc_{eff,0}}$ be the corresponding optimal effective Q-function. By assumption, $A_{eff}^*(s_0; \boldsymbol{\RewardFunc}_0)$ contains at least two actions, $a_1$ and $a_2$, which means $Q_{eff,0}(s_0, a_1) = Q_{eff,0}(s_0, a_2)$.

We define a continuous "bump" function $\varphi \in \Cont(\StateSpace \times \ActionSpace)$ as Proposition~\ref{prop:discontinuity_deterministic} such that:
\begin{itemize}
    \item $0 \le \varphi(s,a) \le 1$ for all $(s,a)$.
    \item $\varphi(s_0, a_2) = 1$.
    \item $\varphi(s_0, a_j) = 0$ for all other actions $a_j \in A_{eff}^*(s_0; \boldsymbol{\RewardFunc}_0)$.
\end{itemize}
For any $\varepsilon > 0$, we define a perturbed effective Q-function $\QFunc_{eff, \varepsilon}$:
\[
\QFunc_{eff, \varepsilon}(s,a) := Q_{eff,0}(s,a) + \varepsilon\,\varphi(s,a).
\]

\noindent\textbf{2. Invert the Bellman Equation to Find the Target Effective Reward}

Using the inverse Bellman mapping defined previously, we find the unique effective reward function, $\RewardFunc_{eff, \varepsilon}$, whose optimal Q-function is exactly $\QFunc_{eff, \varepsilon}$:
\[
\RewardFunc_{eff, \varepsilon} := \RewardFunc_{Q_{eff, \varepsilon}} \quad \text{such that} \quad \OptQFunc_{\RewardFunc_{eff, \varepsilon}} = \QFunc_{eff, \varepsilon}.
\]
As established in the proof of Proposition \ref{prop:discontinuity_deterministic}, we know that as $\varepsilon \to 0$, $\RewardFunc_{eff, \varepsilon}$ converges uniformly to $\RewardFunc_{eff, 0}$.

\noindent\textbf{3. Construct a Convergent Sequence of Reward Tuples}

Now we must construct a sequence of reward tuples, $\boldsymbol{\RewardFunc}_\varepsilon = (\RewardFunc_{1,\varepsilon}, \dots, \RewardFunc_{N,\varepsilon})$, that converges to $\boldsymbol{\RewardFunc}_0$ and generates our target effective reward $\RewardFunc_{eff, \varepsilon}$. Let $\Delta \RewardFunc_\varepsilon = \RewardFunc_{eff, \varepsilon} - \RewardFunc_{eff, 0}$.

We need to find perturbations $\delta_k(s,a) = \RewardFunc_{k, \varepsilon}(s,a) - \RewardFunc_{k,0}(s,a)$ that satisfy two conditions:
\begin{enumerate}
    \item $\sum_{k=1}^N w_k(s) \delta_k(s,a) = \Delta \RewardFunc_\varepsilon(s,a)$ for all $(s,a)$.
    \item $\max_k \SupNorm{\delta_k} \to 0$ as $\varepsilon \to 0$.
\end{enumerate}

We can distribute the perturbation across all components in a simple and robust manner. We define the perturbation for every reward component to be identical to the target perturbation of the effective reward:
\[
\delta_k(s,a) := \Delta \RewardFunc_\varepsilon(s,a) \quad \text{for all } k \in \{1, \ldots, N\}.
\]
So, the new reward tuple $\boldsymbol{\RewardFunc}_\varepsilon$ is defined as:
\[
\RewardFunc_{k, \varepsilon}(s,a) := \RewardFunc_{k,0}(s,a) + \Delta \RewardFunc_\varepsilon(s,a) \quad \text{for all } k.
\]

It's easy to verify that the construction correctly produces the target effective reward. The norm of the difference between the new and old reward tuples is:
\begin{align*}
{\SupNorm{\boldsymbol{\RewardFunc}_\varepsilon - \boldsymbol{\RewardFunc}_0}}_{N} = \max_{k} \SupNorm{\RewardFunc_{k,\varepsilon} - \RewardFunc_{k,0}} = \SupNorm{\Delta \RewardFunc_\varepsilon} = \SupNorm{\RewardFunc_{eff, \varepsilon} - \RewardFunc_{eff, 0}}.
\end{align*}
In the proof of Proposition \ref{prop:discontinuity_deterministic} (which is invoked in Step 2 of this proof), it was established that $\|\RewardFunc_{eff, \varepsilon} - \RewardFunc_{eff, 0}\|_\infty \le \varepsilon(1+\gamma)$. Therefore:
\[
{\SupNorm{\boldsymbol{\RewardFunc}_\varepsilon - \boldsymbol{\RewardFunc}_0}}_{N} \to 0 \quad \text{as } \varepsilon \to 0.
\]

\noindent\textbf{4. The Action Switch and Discontinuity}

We have successfully constructed a sequence of reward tuples $\boldsymbol{\RewardFunc}_\varepsilon \to \boldsymbol{\RewardFunc}_0$. The optimal effective Q-function for $\boldsymbol{\RewardFunc}_\varepsilon$ is $\QFunc_{eff, \varepsilon}$.

The final part of the argument is identical to that in Proposition \ref{prop:discontinuity_deterministic}. By construction, for any $\varepsilon > 0$, $\QFunc_{eff, \varepsilon}(s_0, a_2)$ is strictly greater than $\QFunc_{eff, \varepsilon}(s_0, a')$ for any other action $a'$. Thus, $a_2$ is the unique optimal action for the effective reward.
$A_{eff}^*(s_0; \boldsymbol{\RewardFunc}_\varepsilon) = \{a_2\}$.

\textbf{(1) Deterministic Case:} If the selection rule for $\boldsymbol{\RewardFunc}_0$ chose $a_1$, the map $M_{RL}^{eff}$ must now select $a_2$ for any $\boldsymbol{\RewardFunc}_\varepsilon$. The policy jumps from $a_1$ to $a_2$, proving discontinuity.

\textbf{(2) Stochastic Case:} For $\boldsymbol{\RewardFunc}_0$, the policy at $s_0$ is a uniform distribution over $A_{eff}^*(s_0; \boldsymbol{\RewardFunc}_0)$, which has mass $\le 1/2$ at $a_2$. For any $\boldsymbol{\RewardFunc}_\varepsilon$, the policy becomes a Dirac delta measure at $a_2$, with mass 1. The Total Variation distance between these policies is constant and non-zero. Thus, the map to the stochastic policy is also discontinuous.
\end{proof}


\subsection{Mitigating Discontinuities: Entropy Regularization}\label{subsec:Entropy_Regularization}

Entropy regularization offers another powerful mechanism to promote policy smoothness and address the degeneracy of optima (see e.g., \cite{haarnoja2018soft,geist2019theory}). By adding an \textit{entropy bonus}, it acts as a \textit{tie-breaker} that encourages the policy to be stochastic, especially among actions with similar values. This resolves policy indeterminacy by yielding a unique, smooth optimal policy.

The objective effectively optimized with entropy regularization~\citep{geist2019theory} would be:
\begin{equation}
J_{eff, \alpha}(\Policy; \boldsymbol{\RewardFunc}) = \mathbb{E}_{\Policy} \left[ \sum_{t=0}^\infty \gamma^t \left( \RewardFunc_{eff}(s_t, a_t; \boldsymbol{\RewardFunc}) + \alpha H(\Policy(\cdot|s_t)) \right) \right],
\end{equation}
where $H(\Policy(\cdot|s)) = -\sum_a \Policy(a|s) \log \Policy(a|s)$ is the policy entropy at state $s$, $\alpha > 0$ is the temperature, and the expectation is over trajectories generated by $\Policy$ starting from an appropriate initial state distribution, with rewards $\RewardFunc_{eff}$.

The corresponding soft Bellman optimality operator for $\OptQFunc_{eff,\alpha}(s,a; \boldsymbol{\RewardFunc})$ is:
\begin{equation}
(\BellmanOp_{eff, \boldsymbol{\RewardFunc},\alpha} Q)(s,a) = \RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}) + \gamma \mathbb{E}_{s' \sim \TransKernel(\cdot|s,a)} \left[ \alpha \log \sum_{a' \in \ActionSpace} \exp(Q(s',a')/\alpha) \right].
\end{equation}
For $\alpha > 0$, $\OptQFunc_{eff,\alpha}$ is unique with respect to $\RewardFunc_{eff}$ (and thus with respect to $\boldsymbol{\RewardFunc}$) under standard conditions. The optimal regularized policy, $\OptPolicy_{\boldsymbol{\RewardFunc},\alpha}$, takes the Boltzmann (softmax) form:
\begin{equation}
\OptPolicy_{\boldsymbol{\RewardFunc},\alpha}(a|s) = \frac{\exp(\OptQFunc_{eff,\alpha}(s,a; \boldsymbol{\RewardFunc})/\alpha)}{\sum_{b \in \ActionSpace} \exp(\OptQFunc_{eff,\alpha}(s,b; \boldsymbol{\RewardFunc})/\alpha)}.
\end{equation}
This policy is unique with respect to $\boldsymbol{\RewardFunc}$. See e.g., \cite{geist2019theory} for more information.


\begin{proposition}[Lipschitz Continuity of Soft Optimal Policy]\label{prop:soft-policy-lipschitz-eff}
For $\alpha > 0$, under Assumptions \ref{assump:spaces}-\ref{assump:discount} and the existence of continuous $w_k(s)$ summing to 1, the mapping $\boldsymbol{\RewardFunc} \mapsto \OptPolicy_{\boldsymbol{\RewardFunc},\alpha}$ is Lipschitz continuous from $(\RewardSpace^N, {\SupNorm{\cdot}}_{N})$ to the space of policies. Specifically, for each $s \in \StateSpace$:
\[
d_{TV}(\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s), \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)) \le \frac{1}{2\alpha(1-\gamma)} {\SupNorm{\boldsymbol{\RewardFunc}_1 - \boldsymbol{\RewardFunc}_2}}_{N}.
\]
\end{proposition}

\begin{proof}
Let $\boldsymbol{\RewardFunc}_1, \boldsymbol{\RewardFunc}_2 \in \RewardSpace^N$.
Let $\RewardFunc_{eff,1}(s,a) = \RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}_1)$ and $\RewardFunc_{eff,2}(s,a) = \RewardFunc_{eff}(s,a; \boldsymbol{\RewardFunc}_2)$ be the corresponding effective reward functions as defined in Eq.~\eqref{eq:effective_reward}.
Let $\OptQFunc_1(s,a) = \OptQFunc_{eff,\alpha}(s,a; \boldsymbol{\RewardFunc}_1)$ and $\OptQFunc_2(s,a) = \OptQFunc_{eff,\alpha}(s,a; \boldsymbol{\RewardFunc}_2)$ be the unique fixed points of the respective soft Bellman optimality operators:
\begin{align*}
\OptQFunc_1(s,a) &= \RewardFunc_{eff,1}(s,a) + \gamma \mathbb{E}_{s' \sim \TransKernel(\cdot|s,a)} \left[ \alpha \log \sum_{a'} \exp(\OptQFunc_1(s',a')/\alpha) \right], \\
\OptQFunc_2(s,a) &= \RewardFunc_{eff,2}(s,a) + \gamma \mathbb{E}_{s' \sim \TransKernel(\cdot|s,a)} \left[ \alpha \log \sum_{a'} \exp(\OptQFunc_2(s',a')/\alpha) \right].
\end{align*}
The proof proceeds as follows:

\noindent\textbf{1. Bound the difference in effective rewards.}
From Lemma \ref{lemma:effective_reward_properties}, we have:
\begin{equation}\label{eq:proof_Reff_diff}
\SupNorm{\RewardFunc_{eff,1} - \RewardFunc_{eff,2}} \le {\SupNorm{\boldsymbol{\RewardFunc}_1 - \boldsymbol{\RewardFunc}_2}}_{N}.
\end{equation}

\noindent\textbf{2. Bound the difference in optimal soft Q-functions.}
The soft Bellman optimality operator $T_{R,\alpha}Q = R + \gamma \mathcal{L}_\alpha Q$, where $(\mathcal{L}_\alpha Q)(s,a) = \mathbb{E}_{s' \sim \TransKernel(\cdot|s,a)} [\alpha \log \sum_{a'} \exp(Q(s',a')/\alpha)]$, is a contraction mapping with modulus $\gamma$ in the supremum norm~\citep{geist2019theory}. That is, $\SupNorm{T_{R,\alpha}Q_A - T_{R,\alpha}Q_B} \le \gamma \SupNorm{Q_A - Q_B}$.
Let $Q_1 = \OptQFunc_1$ and $Q_2 = \OptQFunc_2$. Then $Q_1 = T_{\RewardFunc_{eff,1},\alpha} Q_1$ and $Q_2 = T_{\RewardFunc_{eff,2},\alpha} Q_2$.
Consider $\SupNorm{Q_1 - Q_2}$:
\begin{align*}
\SupNorm{Q_1 - Q_2} &= \SupNorm{T_{\RewardFunc_{eff,1},\alpha} Q_1 - T_{\RewardFunc_{eff,2},\alpha} Q_2} \\
&\le \SupNorm{T_{\RewardFunc_{eff,1},\alpha} Q_1 - T_{\RewardFunc_{eff,1},\alpha} Q_2} + \SupNorm{T_{\RewardFunc_{eff,1},\alpha} Q_2 - T_{\RewardFunc_{eff,2},\alpha} Q_2}.
\end{align*}
Using the contraction property for the first term and direct evaluation for the second:
$(\BellmanOp_{\RewardFunc_{eff,1},\alpha} Q_2)(s,a) - (\BellmanOp_{\RewardFunc_{eff,2},\alpha} Q_2)(s,a) = \RewardFunc_{eff,1}(s,a) - \RewardFunc_{eff,2}(s,a)$.
So, $\SupNorm{T_{\RewardFunc_{eff,1},\alpha} Q_2 - T_{\RewardFunc_{eff,2},\alpha} Q_2} = \SupNorm{\RewardFunc_{eff,1} - \RewardFunc_{eff,2}}$.
Thus,
\[ \SupNorm{Q_1 - Q_2} \le \gamma \SupNorm{Q_1 - Q_2} + \SupNorm{\RewardFunc_{eff,1} - \RewardFunc_{eff,2}}. \]
Rearranging gives:
\[ (1-\gamma) \SupNorm{Q_1 - Q_2} \le \SupNorm{\RewardFunc_{eff,1} - \RewardFunc_{eff,2}}. \]
So,
\begin{equation}\label{eq:proof_Q_diff}
\SupNorm{Q_1 - Q_2} \le \frac{1}{1-\gamma} \SupNorm{\RewardFunc_{eff,1} - \RewardFunc_{eff,2}}.
\end{equation}
Combining with Eq.~\eqref{eq:proof_Reff_diff}:
\begin{equation}\label{eq:proof_Q_diff_final}
\SupNorm{Q_1 - Q_2} \le \frac{1}{1-\gamma} {\SupNorm{\boldsymbol{\RewardFunc}_1 - \boldsymbol{\RewardFunc}_2}}_{N}.
\end{equation}

\noindent\textbf{3. Bound the Total Variation distance between policies.} 
For a fixed state $s$, we view the Q-function as a vector of values over the action space $\ActionSpace$. Let the cardinality of $\ActionSpace$ be $d$. The optimal soft policy is the softmax function applied to this Q-value vector, i.e., 
$$\OptPolicy_{\boldsymbol{\RewardFunc},\alpha}(\cdot|s) = \text{softmax}(\QFunc(s, \cdot)/\alpha) \in \R^d.$$
We aim to bound the total variation distance $d_{TV}(\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s), \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s))$, which is defined as:
$$d_{TV}(\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s), \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)) = \frac{1}{2} \sum_{a \in \ActionSpace} |\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(a|s) - \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(a|s)| = \frac{1}{2} \|\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s) - \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)\|_{1}.$$
To achieve this, we apply Lemma \ref{lem:Lipschitz_properties_softmax} in Appendix, which establishes the Lipschitz continuity of the softmax function. The lemma states that for any two vectors $x, y \in \R^d$:
$$\|\text{softmax}(x/\alpha) - \text{softmax}(y/\alpha)\|_1 \le \frac{1}{\alpha} \|x - y\|_{\infty}.$$
We apply this result by setting $x$ and $y$ to be the Q-value vectors at state $s$. This yields the following inequality for the $L_1$-distance between the policies at state $s$:
$$\|\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s) - \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)\|_{1} \le \frac{1}{\alpha} \|\QFunc_1(s, \cdot) - \QFunc_2(s, \cdot)\|_{\infty},$$
where $\|\QFunc_1(s, \cdot) - \QFunc_2(s, \cdot)\|_{\infty} = \max_{a \in \ActionSpace} |\QFunc_1(s, a) - \QFunc_2(s, a)|$.

Since the maximum difference at a single state $s$ is bounded by the supremum norm over all states and actions (i.e., $\max_{a \in \ActionSpace} |\QFunc_1(s, a) - \QFunc_2(s, a)| \le \SupNorm{\QFunc_1 - \QFunc_2}$), we have:
\begin{equation}\label{eq:proof_policy_diff_Q}
d_{TV}(\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s), \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)) \le \frac{1}{2} \left( \frac{1}{\alpha} \SupNorm{\QFunc_1 - \QFunc_2} \right) = \frac{1}{2\alpha} \SupNorm{\QFunc_1 - \QFunc_2}.
\end{equation}

\noindent\textbf{4. Combining all steps.}
Substitute Eq.~\eqref{eq:proof_Q_diff_final} into Eq.~\eqref{eq:proof_policy_diff_Q}:
\begin{align*}
d_{TV}(\OptPolicy_{\boldsymbol{\RewardFunc}_1,\alpha}(\cdot|s), \OptPolicy_{\boldsymbol{\RewardFunc}_2,\alpha}(\cdot|s)) &\le \frac{1}{2\alpha} \left( \frac{1}{1-\gamma} {\SupNorm{\boldsymbol{\RewardFunc}_1 - \boldsymbol{\RewardFunc}_2}}_{N} \right) \\
&= \frac{1}{2\alpha(1-\gamma)} {\SupNorm{\boldsymbol{\RewardFunc}_1 - \boldsymbol{\RewardFunc}_2}}_{N}.
\end{align*}
This establishes the Lipschitz continuity with the constant $C(\alpha,\gamma) = \frac{1}{2\alpha(1-\gamma)}$. This constant depends only on $\alpha$ and $\gamma$, and not on the state $s$. 
\end{proof}

\input{figures/soft_policy.tex}

While entropy regularization, yielding a softmax optimal policy, endows the reward-policy map with desirable continuity (as demonstrated by its Lipschitz continuity properties, i.e., Proposition \ref{prop:soft-policy-lipschitz-eff}) and can enhance training stability, these advantages are accompanied by inherent trade-offs. The principal cost is a degree of \textit{suboptimality with respect to the immediate reward function}; by distributing probability mass across multiple actions rather than exclusively selecting the action with the highest estimated value, the policy intentionally sacrifices some exploitative potential for increased stochasticity and exploration. Figure~\ref{fig:soft_policy} provides a conceptual illustration of hard-max and softmax policy behaviors. 

This characteristic can manifest behaviorally as a \textit{"blurring" or "averaging"} effect, where the policy may produce less decisive or distinctive outputs. This might be suboptimal for tasks demanding high-precision or optimal responses. Furthermore, the interaction of a softmax policy with an imperfect or incomplete reward model can influence the manifestation of undesirable behaviors, such as \textit{hallucinations in large language models}. While not a direct cause, the policy's inherent stochasticity means it may assign non-trivial probability to actions leading to subtly flawed or fabricated content if these actions possess near-optimal, yet inadequately penalized, values under the effective reward function ($\RewardFunc_{eff}$). The extent of these trade-offs is critically governed by the temperature parameter $\alpha$, which necessitates careful calibration to balance policy smoothness and exploration against the fidelity of exploiting the reward landscape.




\subsection{Implications for Multi-Task RL Training for LLMs}

The preceding analysis, which centers on policies derived from a state-dependent effective reward model, $\RewardFunc_{eff}$, yields critical insights into the stability of LLMs trained with multiple specialized reward models. The stability of the resultant policy, ${\OptPolicy_{\boldsymbol{\RewardFunc}}}^{eff}$, is fundamentally governed by the properties of this constructed effective reward and its associated optimal Q-function, $\OptQFunc_{eff}$.

The construction of the effective reward, particularly through the state-dependent weights $w_k(s)$, emerges as a decisive factor. An unstable or ill-defined weighting mechanism, or significant conflict between the specialized rewards $\RewardFunc_k$, may render $\RewardFunc_{eff}$ highly sensitive to perturbations in any individual reward component $\RewardFunc_j$. This sensitivity is a primary source of policy instability. Specifically, if the resulting $\OptQFunc_{eff}$ exhibits non-unique optimal actions for a given reward tuple $\boldsymbol{\RewardFunc}_0$, the system becomes susceptible to discontinuous behavior. As demonstrated, a minor change to underlying reward models can alter the effective reward landscape sufficiently to break ties within the optimal action set $A_{eff}^*$, causing an abrupt shift in the selected policy.

Conversely, entropy regularization provides a robust mechanism for ensuring policy stability within this framework. By incorporating an entropy bonus into the optimization objective with respect to $\RewardFunc_{eff}$, this method smooths the policy landscape. It guarantees the existence of a unique, stochastic optimal policy, $\OptPolicy_{\boldsymbol{\RewardFunc},\alpha}$, that varies continuously with the underlying reward tuple $\boldsymbol{\RewardFunc}$. This approach can therefore substantially enhance the behavioral stability of an LLM against modifications in its constituent reward models, albeit at the cost of a less sharply optimized policy for any temperature $\alpha > 0$.

Ultimately, these findings underscore that the strategy by which an LLM integrates multiple reward signals across diverse domains, modeled here via the effective reward mechanism, is central to its stability properties. While our analysis has focused on the continuity of policies optimal for a given $\RewardFunc_{eff}$, a key practical challenge remains: to design or learn an aggregation mechanism, i.e., the weights $w_k(s)$, such that the resulting policy not only exhibits desirable stability but also effectively optimizes the intended global performance measure $J(\Policy)$.


\subsection{Remarks on State-Dependent Aggregation Mechanism}
\label{subsec:wk_theoretical_basis}

A crucial assumption underpinning our analysis is that the aggregation weights $w_k(s)$ are given, continuous functions of the state $s$, and remain fixed during the perturbation of the reward tuple $\boldsymbol{\RewardFunc}$. This simplification is necessary for analytical tractability. In a more general setting, these weights might themselves depend on the policy $\Policy$ or the reward models $\boldsymbol{\RewardFunc}$, leading to an ideal but far more complex formulation, $w_k^*(s; \Policy, \boldsymbol{\RewardFunc})$.

Such dependencies, while realistic, would introduce significant analytical challenges. First, if $w_k(s)$ were a function of $\Policy$, the reward function in the Bellman equation would become policy-dependent, violating the foundational assumptions of the standard MDP framework. Second, a dependency of $w_k(s)$ on $\boldsymbol{\RewardFunc}$ would compromise the continuity of the effective reward map itself, invalidating the Lipschitz properties established in Lemma \ref{lemma:effective_reward_properties} and altering all subsequent stability results.

Consequently, our framework is best interpreted as modeling systems where the aggregation mechanism is structurally fixed. This assumption corresponds to several plausible implementation scenarios. For instance, the weights could represent a simplified, policy-independent heuristic for task relevance, such as using prior probabilities $p_k$. Alternatively, and more compellingly, it can model a sophisticated architecture with a separate, pre-trained, and now-frozen component, such as a context encoder or a gating network, that determines task relevance based on the input state. In this view, our analysis assesses the stability of a downstream policy optimization process with respect to its reward models, given a fixed routing or aggregation module.

By adopting this deliberate simplification, we can employ the standard Bellman machinery to analyze a well-defined effective MDP. This provides clear and rigorous insights into how reward perturbations propagate through a structured, yet representative, multi-task decision-making model. The broader question concerning the stability of jointly learning the aggregation weights and the policy would necessitate a more extensive analytical framework beyond the scope of this work.
