\section{Introduction}\label{sec-intro}

The development of a new generation of large language models (LLMs) represents a significant advance in artificial intelligence. Systems such as  OpenAI's o-series (o1, o3, o4-mini)~\citep{openai2024o1,openai2025o3o4}, Google's Gemini 2.5~\citep{gemini2025frontier}, Anthropic's Claude 4~\citep{anthropic2025claude} and xAI's Grok 4 are increasingly designed not merely as conversational agents, but as large reasoning models (LRMs) capable of addressing complex, multi-step problems across domains like mathematics, science, and software engineering. The development of these systems, alongside powerful open-source models like Deepseek R1~\citep{guo2025deepseek} and Qwen3~\citep{yang2025qwen3}, relies heavily on reinforcement learning (RL) as a crucial training methodology. RL is employed not only to align model behavior with safety and ethical standards but also to teach models how to generate sophisticated reasoning trajectories~\citep{guo2025deepseek,yang2025qwen3}. In this way, RL plays a pivotal role in both scaling complex reasoning capabilities and ensuring robust alignment with human values.

Yet despite its promise, the application of RL introduces a set of persistent challenges that undermine both reasoning quality and alignment stability. At the heart of these difficulties lies the reliance on optimizing model policies against learned or specified reward functions. RL-trained policies often exhibit brittleness and undesirable generalization. For example, a model rewarded solely for a correct final answer, with no credit for a valid reasoning process, may resort to spurious reasoning: learning to generate the answer via a shortcut, then fabricating a plausible-but-fallacious justification post-hoc~\citep{baker2025monitoring,wang2025persona,chen2025reasoning}. In other cases, policies can degrade in unpredictable ways. A model intended to be an objective assistant might exhibit a sudden style shift and adopt an overly sycophantic tone~\citep{openai2025expanding,openai2025sycophancy}. A sharp degradation in instruction-following fidelity is also common~\citep{fu2025scaling}, where models ignore specified output formats, length constraints, or language requirements when such details are not strongly enforced by the reward. These failure modes are more than just cosmetic---they threaten the reliability, controllability, and safety of large-scale AI systems.

This challenge highlights a fundamental distinction between applying RL to language models and its use in traditional domains. In settings like Go or chess~\citep{silver2016mastering,silver2017mastering,silver2018general}, where rewards are well-defined and objective, the policy is simply a tool to maximize outcomes; the path taken matters little as long as the goal is achieved. But for LLMs, this paradigm no longer holds. The output sequence itself is the product, directly consumed by users. Moreover, the reward is not an objective truth but a noisy approximation of human preferences~\citep{ouyang2022training,bai2022constitutional,lee2023rlaif} or manually specified rules~\citep{wang2025reinforcement,su2025crossing}. As a result, the policyâ€™s behavior cannot be treated as a secondary concern---it is central. In language modeling, the policy is not just a means to an end; in many ways, it is the end.

This shift in paradigm reframes the observed instabilities: they are not superficial flaws, but fundamental failures in the product itself. This raises a deeper and largely unresolved question: \textbf{What underlying mathematical mechanisms make RL-trained policies so sensitive to the design of the reward function in both LLM reasoning and alignment?} In the absence of a formal theory of stability, current solutions rely heavily on ad-hoc heuristics and empirical tuning rather than first principles. Common practices include reward engineering or algorithmic modifications, such as adding specific penalties to suppress issues like spurious reasoning, poor instruction adherence, or inefficient problem-solving~\citep{baker2025monitoring,wang2025persona,chen2025reasoning,fu2025scaling,sui2025stop,aggarwal2025l1,chen2025towards,qu2025survey}. Additional strategies include extensive preference data curation~\citep{ouyang2022training} and the use of structured but non-theoretical frameworks such as Constitutional AI~\citep{bai2022constitutional} or deliberative alignment~\citep{guan2024deliberative} to guide the learning. While these approaches can mitigate specific failure modes, they are inherently reactive and lack a general understanding or guarantee of policy stability.

To build a principled foundation, this paper develops a rigorous mathematical framework for analyzing the stability of the mapping from reward functions to their corresponding optimal policies. By modeling reasoning trace generation as a Markov Decision Process (MDP) and applying tools from functional analysis, we examine the continuity of this reward-to-policy map to formally explain the roots of policy brittleness. Our analysis shows that instabilities often arise from two key sources: non-unique optimal actions and imprecise reward signals. When a reward model assigns nearly equal value to multiple distinct actions, a common scenario in the expansive space of reasoning and creative generation, the resulting policy becomes inherently unstable. In such cases, even tiny perturbations in the reward can lead to abrupt, discontinuous changes in behavior, revealing a fundamental fragility at the heart of RL-trained language models.

Building on this theoretical foundation, we apply our framework to analyze a wide range of observed LLM behaviors through two key lenses: incomplete reward specifications and tie-breaking dynamics among degenerate optima. Many failure cases can be understood as manifestations of the "clever slacker" problem, where the policy rationally exploits an underspecified reward. For example, spurious reasoning emerges when only the final answer is rewarded, giving no incentive for a coherent reasoning process. Likewise, instruction-following failures, such as ignoring output format, length, or language constraints, occur when these aspects are not reflected in the reward except for correctness. Our framework also clarifies the role of auxiliary rewards. By breaking ties between near-equally valued actions, even small penalties (e.g., for verbosity) or bonuses (e.g., for correct formatting) can shift the policy toward desirable behaviors. \textit{These discontinuous shifts are not inherently undesirable---they can be leveraged as mechanisms for reward shaping, allowing designers to steer policies when the base reward is incomplete or noisy.} Ultimately, our analysis offers a unified mathematical perspective on these phenomena, showing they arise naturally from the structure of the reward landscape and the sensitivity of policy optimization within it.

While our single-reward analysis yields valuable insights, leading LLMs such as OpenAI's o-series, Gemini 2.5, and Grok 4 are rarely trained on a singular reward. Instead, they are typically optimized using a complex blend of reward signals across multiple domains, ranging from mathematical reasoning and code generation to safety alignment~\citep{guo2025deepseek,liang2025modomodo,cheng2025revisiting}. This motivates the second stage of our analysis, where we extend the framework to more realistic multi-reward training regimes.

In this setting, we model the system's behavior using a state-dependent effective reward function that captures how the model internally aggregates multiple (and sometimes conflicting) objectives. We demonstrate that the same principles of stability apply: policy continuity now depends on the properties of this effective reward. Crucially, this highlights the aggregation mechanism of the effective reward as a key determinant of policy robustness. To address resulting instabilities, we formally analyze the role of regularization techniques and show that entropy regularization restores Lipschitz continuity in the reward-policy map. This guarantees that small changes in the reward yield correspondingly small changes in behavior, but it comes at the cost of trading off some degree of optimality for greater stability.

To validate the practical relevance of our framework, we systematically link its theoretical insights to a broad spectrum of recent empirical findings. Our analysis accounts for the deceptive reasoning behavior~\citep{wang2025persona,baker2025monitoring}: from simple cheating under a weak reward model to a more pernicious policy shift towards obfuscated deception when that reward is naively "patched". We further elucidate the intelligence-obedience trade-off in large reasoning models~\citep{fu2025scaling} and demonstrate how targeted auxiliary rewards can serve as tie-breakers to resolve such tensions and enable fine-grained behavioral control~\citep{aggarwal2025l1}. Extending to the subjective realm of RLHF, our theory also explains the emergence of "performance illusions"~\citep{wen2024language}, where models shift from truthful responses to persuasive but misleading ones that exploit human feedback biases. Finally, we review and present results related to the multi-reward setting. These case studies provide comprehensive empirical support for our framework, offering a unified lens through which to understand critical stability challenges in modern AI.

Our key contributions are as following:
\begin{enumerate}
\item \textbf{A theoretical framework for policy stability in RL-trained language models.} We establish a formal analysis of the reward-policy map and prove that policy instability stems from inherent properties of the reward landscape, notably reward misspecification and the presence of degenerate optima.

\item \textbf{A unified explanation for alignment and reasoning phenomena in LLMs.} We demonstrate that common failure modes, such as spurious reasoning, poor instruction-following and inefficient reasoning, emerge naturally from misspecified reward signals. We also introduce the notion of an effective reward as a key determinant of policy robustness in a multi-reward setting.

\item \textbf{A principled justification for entropy regularization.} We prove that entropy regularization restores continuity in the reward-policy map, providing a theoretical foundation for its widespread use in stabilizing behavior during large-scale model training.

\end{enumerate}

