\section{Discussion}\label{sec-discussion}

Our work has established a formal mathematical framework for analyzing the stability of the reward-policy map in reinforcement learning. While the preceding sections have demonstrated its power in explaining and unifying a range of critical phenomena in LLM alignment and reasoning, it is equally important to discuss the assumptions underpinning our theory and its relationship with the complexities of real-world training. This section addresses three key aspects: the continuity assumption of the reward function, the influence of practical optimization algorithms, and the role of data distribution.

\subsection{On the Assumption of Reward Continuity}

Our theoretical framework is built upon the assumption that the reward function \(\RewardFunc \in \Cont(S \times A)\). As established in our framing of the LLM problem, for a finite state-action space, any function is formally continuous due to the discrete topology induced by any metric. This means that even the binary success/failure signals common in practice, such as \(\RewardFunc(s,a) \in \{0,1\}\), satisfy this topological assumption. Therefore, the source of policy instability in the LLM setting does not stem from a formal "discontinuity" of the reward function itself.

The critical issue with such sparse, outcome-based rewards lies not in their topology, but in their functional impact on the value landscape: they are a primary driver of \textit{degeneracy}. Because a vast number of distinct behavioral trajectories can lead to the same binary outcome (e.g., a correct final answer), they are all assigned an identical reward value. This equality propagates through the Bellman equation, resulting in an optimal Q-function, \(Q^*_\RewardFunc\), where many different actions at a given state share the exact same maximum value.

This widespread degeneracy creates the precise conditions under which the policy map becomes unstable. It enlarges the set of optimal actions, \(A^*(s; \RewardFunc)\), and flattens the value landscape, making the \(\Argmax\) correspondence acutely sensitive to any perturbation in the Q-function's values. Consequently, while binary rewards do not violate the continuity assumption in the finite LLM setting, they drastically increase the prevalence of co-optimal actions. This exacerbates the inherent discontinuity of the \(\Argmax\) operator, making the policy cliffs identified by our theory more pronounced and severe in practice. The instability arises not from a "discontinuous" reward function, but from an optimization process breaking ties among a massively degenerate set of optimal choices and perturbations of reward function.


\subsection{The Role of Practical RL Algorithms and Optimization Dynamics}

Our framework analyzes the stability of the true optimal policy, $\pi^*$, providing a clean understanding of a problem's inherent structural properties. In practice, however, policies are found via iterative algorithms like PPO, which introduce a crucial dynamic that our static analysis does not capture: the perpetual noise inherent in the optimization process. This noise, arising from mini-batch sampling and stochastic gradient updates, acts as a constant source of perturbation. While our theory explains why a degenerate reward landscape makes a policy fundamentally fragile, this algorithmic noise is the ever-present force that can exploit this fragility, preventing the policy from settling into a stable strategy and instead causing the kind of training drift and run-to-run variance commonly observed in practice. A full analysis of these joint effects remains a critical direction for future work.

\subsection{The Influence of Data Distribution and Mixture Ratios}

The stability of a learned policy is not only a function of the reward and the algorithm, but also of the data distribution on which it is trained. Our theoretical framework, by focusing on the properties of the MDP, implicitly assumes access to all states, yet in reality, a policy's behavior is shaped only by the states it encounters during training. Besides, in the multi-reward setting, the mixture ratios directly form the "effective reward" landscape the policy optimizes against. A model trained on a data mixture heavily skewed towards one task (e.g., 90\% coding data) will learn an internal reward aggregation strategy specialized for that task. If this specialized policy is then deployed in an environment with a different task mixture, its behavior can become unstable and unpredictable, as its internal reward strategy is now fundamentally mismatched to the new context. Therefore, the data mixture ratio is not merely a sampling detail but a top-level hyperparameter that governs the structure of the effective reward landscape and, consequently, the stability of the final multi-task model.

